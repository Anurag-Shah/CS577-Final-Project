{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "577FinalProject.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Place the two data files (multi_class.csv, multi_label.csv) in your drive /577FinalProject, or rename the directory."
      ],
      "metadata": {
        "id": "SLkvDz4Ujw-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If this isn't being run on colab you probably want to skip this cell, and then change input dir accordingly\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "IhypNXVpj8gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "QzQUhqWnkVsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If this isn't being run on colab you probably want to skip this cell\n",
        "!pip install gensim\n",
        "!pip install nltk\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "xw4mEpPFks8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
        "import nltk\n",
        "import time\n",
        "\n",
        "nltk.download('stopwords')\n",
        "device = torch.device(\"cuda:0\")"
      ],
      "metadata": {
        "id": "atSWQfJJkaIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters and Globals"
      ],
      "metadata": {
        "id": "3Psfb0-CwGYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classes\n",
        "# ðŸ˜‚, ðŸ¤£, ðŸ˜… are included in the report. \n",
        "#classes = ['â¤', 'ðŸ˜‚', 'ðŸ‘', 'ðŸ¤£', 'ðŸ˜…']\n",
        "classes = ['ðŸ˜­', 'ðŸ˜‚', 'ðŸ˜”', 'ðŸ¤£', 'ðŸ˜…']\n",
        "\n",
        "# Word2Vec Embeddings\n",
        "W2V_VEC_SIZE = 300\n",
        "W2V_EPOCHS = 100\n",
        "W2V_MIN_COUNT = 3 # minimum word count to be included in final embeddings\n",
        "\n",
        "# BERT Embeddings\n",
        "BERT_ENTRIES_PER_CLASS = 1000\n",
        "BERT_CLASS_EQ_FLAG = False\n",
        "BERT_BATCH_SIZE = 32\n",
        "\n",
        "# BOW\n",
        "BOW_ENTRIES_PER_CLASS = 1000\n",
        "BOW_VEC_SIZE = 500\n",
        "\n",
        "# General\n",
        "input_dir = '/content/drive/MyDrive/577FinalProject'"
      ],
      "metadata": {
        "id": "gPwhTEeZwHxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identify Classes"
      ],
      "metadata": {
        "id": "7jnhTtod82CR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only run this cell if you want to update the class list for any reason\n",
        "# It will provide a list of classes, ranked by number of tweets\n",
        "full_dataset = pd.read_csv(f\"{input_dir}/multi_class.csv\")\n",
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "    print(full_dataset['Multi-Class Annotation'].value_counts())\n",
        "del full_dataset"
      ],
      "metadata": {
        "id": "zDVojs7U81iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and Preprocess Dataset for Word2Vec"
      ],
      "metadata": {
        "id": "Mutb8Eihk7kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopwords\n",
        "stopwords = [] + nltk.corpus.stopwords.words('english')\n",
        "stopwords += ['[MENTION]', '[URL', '[HASHTAG]', 'MENTION', 'URL', 'HASHTAG', 'RT']"
      ],
      "metadata": {
        "id": "bF4nUu4Ym-hK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(dataset):\n",
        "    # Tokenizer removes punctuation\n",
        "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "    for i in range(len(dataset)):\n",
        "        item = dataset.iloc[i, 0]\n",
        "        # Since this just updates the raw dataset, isinstance lets us skip the\n",
        "        # cleaning process if accidentally run a second time\n",
        "        if isinstance(item, str):\n",
        "            tokens = tokenizer.tokenize(item)\n",
        "            out = []\n",
        "            for token in tokens:\n",
        "                if token.lower() not in stopwords:\n",
        "                    out.append(token.lower())\n",
        "            dataset.at[i,'Tweet'] = out\n",
        "        if i % 10000 == 0:\n",
        "            print(f\"{i}/{len(dataset)}\")"
      ],
      "metadata": {
        "id": "GpvJGWNPoUyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell only if full_dataset_w2v.csv exists\n",
        "full_dataset_w2v = pd.read_csv(f\"{input_dir}/full_dataset_w2v.csv\")\n",
        "full_dataset_w2v = (full_dataset_w2v[full_dataset_w2v['Multi-Class Annotation'].isin(classes)]).reset_index(drop=True)\n",
        "# train_data = full_dataset_w2v[full_dataset_w2v['Dataset'] == 'train']\n",
        "# test_data = full_dataset_w2v[full_dataset_w2v['Dataset'] == 'test']\n",
        "# val_data = full_dataset_w2v[full_dataset_w2v['Dataset'] == 'dev']\n",
        "\n",
        "full_dataset_w2v"
      ],
      "metadata": {
        "id": "7Au3sX_lCbmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell if you want to rebuild cleaned csv\n",
        "# If full_dataset_w2v.csv already exists, run the previous cell to load it\n",
        "\n",
        "# full_dataset_w2v = pd.read_csv(f\"{input_dir}/multi_class.csv\")\n",
        "full_dataset_w2v = pd.read_csv(f\"577FinalProject/multi_class.csv\")\n",
        "full_dataset_w2v = (full_dataset_w2v[full_dataset_w2v['Multi-Class Annotation'].isin(classes)]).reset_index(drop=True)\n",
        "clean_data(full_dataset_w2v)\n",
        "# train_data = full_dataset_w2v[full_dataset_w2v['Dataset'] == 'train']\n",
        "# test_data = full_dataset_w2v[full_dataset_w2v['Dataset'] == 'test']\n",
        "# val_data = full_dataset_w2v[full_dataset_w2v['Dataset'] == 'dev']\n",
        "\n",
        "full_dataset_w2v"
      ],
      "metadata": {
        "id": "4ApXVXLEoo7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save W2V Dataset (preprocessed) to CSV"
      ],
      "metadata": {
        "id": "txH5ERDeDGQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset_w2v.to_csv(f\"{input_dir}/full_dataset_w2v.csv\")"
      ],
      "metadata": {
        "id": "owrh9AcODKe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Embeddings (Word2Vec)\n"
      ],
      "metadata": {
        "id": "yWNUTStGuryg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence to vector function\n",
        "def w2v_sentence_vector(sent, w2v):\n",
        "    avg = np.zeros(W2V_VEC_SIZE)\n",
        "    count = 0\n",
        "    for word in sent:\n",
        "        if word.lower() not in stopwords:\n",
        "            try:\n",
        "                np.add(avg, w2v.wv[word], out=avg)\n",
        "            except KeyError:\n",
        "                continue\n",
        "            count += 1\n",
        "    if count == 0:\n",
        "        return avg\n",
        "    return np.true_divide(avg, count)\n",
        "\n",
        "# Build corpus\n",
        "corpus = []\n",
        "for item in full_dataset_w2v.iloc[:,0]:\n",
        "    corpus.append(item)\n",
        "\n",
        "# Train w2v model\n",
        "import time\n",
        "start_time = time.time()\n",
        "w2v = gensim.models.word2vec.Word2Vec(\n",
        "    corpus,\n",
        "    size=W2V_VEC_SIZE,\n",
        "    window=10,\n",
        "    workers=4,\n",
        "    iter=W2V_EPOCHS,\n",
        "    min_count=W2V_MIN_COUNT)\n",
        "print(f\"Training Time: {(time.time() - start_time):.1f} seconds\")\n",
        "\n",
        "# Change datasets to use embeddings\n",
        "\n",
        "# train_w2v = train_data.copy()\n",
        "# test_w2v = test_data.copy()\n",
        "# val_w2v = val_data.copy()\n",
        "# train_w2v['Tweet'] = train_w2v['Tweet'].apply(w2v_sentence_vector, args=[w2v])\n",
        "# test_w2v['Tweet'] = test_w2v['Tweet'].apply(w2v_sentence_vector, args=[w2v])\n",
        "# val_w2v['Tweet'] = val_w2v['Tweet'].apply(w2v_sentence_vector, args=[w2v])\n",
        "full_w2v = full_dataset_w2v.copy()\n",
        "full_w2v['Tweet'] = full_w2v['Tweet'].apply(w2v_sentence_vector, args=[w2v])"
      ],
      "metadata": {
        "id": "jzbQHebuuve-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save dataframes after w2v"
      ],
      "metadata": {
        "id": "p2sJ6Vh6Omqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Old Code\n",
        "#train_w2v.to_csv(f\"{input_dir}/train_w2v.csv\")\n",
        "#test_w2v.to_csv(f\"{input_dir}/test_w2v.csv\")\n",
        "#val_w2v.to_csv(f\"{input_dir}/val_w2v.csv\")\n",
        "\n",
        "# Pickle\n",
        "full_w2v.to_pickle(f\"{input_dir}/w2v_embeddings.pkl\")"
      ],
      "metadata": {
        "id": "Q6B7rcKuOmYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_w2v"
      ],
      "metadata": {
        "id": "ZnoFjUN4AmsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and Preprocess Dataset for BERT"
      ],
      "metadata": {
        "id": "gAu_nNiQ0e_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell only if full_dataset_BERT.csv is in your drive\n",
        "full_dataset_BERT = pd.read_pickle(f\"{input_dir}/full_dataset_BERT.pkl\")\n",
        "full_dataset_BERT = (full_dataset_BERT[full_dataset_BERT['Multi-Class Annotation'].isin(classes)]).reset_index(drop=True)\n",
        "full_dataset_BERT"
      ],
      "metadata": {
        "id": "B0O4XKu4BBvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_bert(dataset):\n",
        "    for i in range(len(dataset)):\n",
        "        item = dataset.iloc[i, 0]\n",
        "        # Since this just updates the raw dataset, isinstance lets us skip the\n",
        "        # cleaning process if accidentally run a second time\n",
        "        if isinstance(item, str):\n",
        "            # return_tensors=\"pt\" causes it to return pytorch tensors instead of lists\n",
        "            dataset.at[i,'Tweet'] = tokenizer_bert.encode_plus(item, padding='max_length', max_length=128, truncation=True, return_tensors=\"pt\")\n",
        "        if i % 10000 == 0:\n",
        "            print(f\"{i}/{len(dataset)}\")"
      ],
      "metadata": {
        "id": "qI-nvEaa1DDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Skip this cell if full_dataset_BERT.csv exists and you want to just load that\n",
        "# If you want to edit tokenizer parameters, run the previous cell and this one\n",
        "full_dataset_BERT = pd.read_csv(f\"{input_dir}/multi_class.csv\")\n",
        "full_dataset_BERT = (full_dataset_BERT[full_dataset_BERT['Multi-Class Annotation'].isin(classes)]).reset_index(drop=True)\n",
        "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "tokenize_bert(full_dataset_BERT)\n",
        "full_dataset_BERT"
      ],
      "metadata": {
        "id": "qTZFqB8q82wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select Entries per class (to avoid Memory issues with Colab)"
      ],
      "metadata": {
        "id": "L2giPXxNieC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell if you want BERT Encodings to have equal entries per class\n",
        "# This takes BERT_ENTRIES_PER_CLASS entries per each class in classes\n",
        "# The above two fields are defined in the hyperparameters cell\n",
        "\n",
        "BERT_CLASS_EQ_FLAG = True\n",
        "\n",
        "# Initialize the partial dataset using classes[0]\n",
        "partial_dataset_BERT = (full_dataset_BERT[full_dataset_BERT['Multi-Class Annotation'].isin([classes[0]])]).head(BERT_ENTRIES_PER_CLASS).reset_index(drop=True)\n",
        "\n",
        "# Now do for every other class in classes\n",
        "for i in range(1, len(classes)):\n",
        "    temp = (full_dataset_BERT[full_dataset_BERT['Multi-Class Annotation'].isin([classes[i]])]).head(BERT_ENTRIES_PER_CLASS).reset_index(drop=True)\n",
        "    partial_dataset_BERT = pd.concat([partial_dataset_BERT, temp])\n",
        "\n",
        "# Reset count and drop unnamed column\n",
        "partial_dataset_BERT = partial_dataset_BERT.reset_index(drop=True)\n",
        "\n",
        "partial_dataset_BERT"
      ],
      "metadata": {
        "id": "2PDMbhpEisBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence Embeddings (BERT)"
      ],
      "metadata": {
        "id": "8RNbyNA54bzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize pretrained BERT model\n",
        "bert = BertModel.from_pretrained('bert-base-cased').to(device)"
      ],
      "metadata": {
        "id": "xJL1BlQrtY_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence Embeddings (BERTForSequenceClassification)\n",
        "This model doesn't actually work very well with clustering for obvious reasons\n",
        "But its less painful to train!"
      ],
      "metadata": {
        "id": "1vLYDtQ_kmgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not run this if you want to use the above bert!\n",
        "#bert = BertModel.from_pretrained('bert-base-cased', num_labels = len(classes)).to(device)"
      ],
      "metadata": {
        "id": "33RXL1UwkmFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train BERT\n",
        "\n",
        "Commented out right now because it requires changing some specific cells earlier on to work\n",
        "Since training has no measurable impact, there's no point changing the pipeline to better accomodate it"
      ],
      "metadata": {
        "id": "HZW4uZlVn2ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def labels_to_numbers(labels):\n",
        "#     out = []\n",
        "#     for item in labels:\n",
        "#         out.append(classes.index(item))\n",
        "#     return out\n",
        "\n",
        "# class Dataset(torch.utils.data.Dataset):\n",
        "#     def __init__(self, encodings, labels=None):\n",
        "#         self.encodings = encodings\n",
        "#         self.labels = labels\n",
        "    \n",
        "#     def __getitem__(self, idx):\n",
        "#         item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "#         if self.labels is not None:\n",
        "#             item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "#         return item\n",
        "    \n",
        "#     def __len__(self):\n",
        "#         return len(self.encodings[\"input_ids\"])\n",
        "\n",
        "# tokenizer_bert = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "# x_train = partial_dataset_BERT['Tweet'].to_numpy().tolist()\n",
        "# x_train_tokenized = tokenizer_bert(x_train, padding=True, truncation=True, max_length=128)\n",
        "\n",
        "# labels = np.array(labels_to_numbers(partial_dataset_BERT['Multi-Class Annotation'].to_numpy()))\n",
        "\n",
        "# train_dataset = Dataset(x_train_tokenized)#, labels)\n",
        "# val_dataset = Dataset(x_train_tokenized)#, labels)"
      ],
      "metadata": {
        "id": "oRIIdOdKn2PZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
        "\n",
        "# # Define Trainer\n",
        "# args = TrainingArguments(\n",
        "#     output_dir=\"output\",\n",
        "#     evaluation_strategy=\"steps\",\n",
        "#     eval_steps=500,\n",
        "#     per_device_train_batch_size=8,\n",
        "#     per_device_eval_batch_size=8,\n",
        "#     num_train_epochs=3,\n",
        "#     seed=0,\n",
        "#     load_best_model_at_end=True,)\n",
        "# trainer = Trainer(\n",
        "#     model=bert,\n",
        "#     args=args,\n",
        "#     train_dataset=train_dataset,\n",
        "#     eval_dataset=val_dataset,\n",
        "#     callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],)\n",
        "\n",
        "# # Train pre-trained model\n",
        "# trainer.train()"
      ],
      "metadata": {
        "id": "cnZaojdG4f7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run BERT"
      ],
      "metadata": {
        "id": "jDaQLj6W4kmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is an example. This lets you fetch the vector embedding for any sentence input that you provide\n",
        "# The sentence input, which is the item in the 'Tweet' column of the tokenized dataframe, contains\n",
        "#   three parts: the first and last are relevant to actually running BERT on the tokenized sentence\n",
        "#   This will give you two output tensors, clustering can possibly use both of them\n",
        "#   Though you can probably get away with using only the second one, thats more relevant for us\n",
        "## input_id = full_dataset_BERT['Tweet'][0]['input_ids']\n",
        "## attention_mask = full_dataset_BERT['Tweet'][0]['attention_mask']\n",
        "## print(bert(input_ids = input_id, attention_mask = attention_mask, return_dict = False))\n",
        "\n",
        "def quick_bert(item):\n",
        "    _, pooled = bert(input_ids=item['input_ids'].to(device), attention_mask=item['attention_mask'].to(device), return_dict=False)\n",
        "    return pooled.cpu().detach().numpy()\n",
        "\n",
        "start_time = time.time()\n",
        "#print(bert(input_ids=partial_dataset_BERT['Tweet'][0]['input_ids'].to(device), attention_mask=partial_dataset_BERT['Tweet'][0]['attention_mask'].to(device), return_dict=False))\n",
        "if (BERT_CLASS_EQ_FLAG):\n",
        "    partial_dataset_BERT['Tweet'] = partial_dataset_BERT['Tweet'].apply(quick_bert)\n",
        "    partial_dataset_BERT\n",
        "else:\n",
        "    full_dataset_BERT['Tweet'] = full_dataset_BERT['Tweet'].apply(quick_bert)\n",
        "    full_dataset_BERT\n",
        "print(time.time() - start_time)\n"
      ],
      "metadata": {
        "id": "h8zeoEZhdUzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save BERT Dataset to csv"
      ],
      "metadata": {
        "id": "YpC8EMvgABBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train_bert = full_dataset_BERT[full_dataset_BERT['Dataset'] == 'train']\n",
        "#test_bert = full_dataset_BERT[full_dataset_BERT['Dataset'] == 'test']\n",
        "#val_bert = full_dataset_BERT[full_dataset_BERT['Dataset'] == 'dev']\n",
        "#train_bert.to_csv(f\"{input_dir}/train_BERT.csv\")\n",
        "#test_bert.to_csv(f\"{input_dir}/test_BERT.csv\")\n",
        "#val_bert.to_csv(f\"{input_dir}/val_BERT.csv\")\n",
        "\n",
        "#full_dataset_BERT.to_pickle(f\"{input_dir}/full_dataset_BERT.pkl\")\n",
        "partial_dataset_BERT.to_pickle(f\"{input_dir}/BERT_Embeddings.pkl\")"
      ],
      "metadata": {
        "id": "slL-XSDdADSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of Words"
      ],
      "metadata": {
        "id": "4vCvB5iEZp4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset_bow = pd.read_csv(f\"{input_dir}/multi_class.csv\")"
      ],
      "metadata": {
        "id": "sJIvxchlbJbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the partial dataset using classes[0]\n",
        "partial_dataset_bow = (full_dataset_bow[full_dataset_bow['Multi-Class Annotation'].isin([classes[0]])]).head(BOW_ENTRIES_PER_CLASS).reset_index(drop=True)\n",
        "\n",
        "# Now do for every other class in classes\n",
        "for i in range(1, len(classes)):\n",
        "    temp = (full_dataset_bow[full_dataset_bow['Multi-Class Annotation'].isin([classes[i]])]).head(BOW_ENTRIES_PER_CLASS).reset_index(drop=True)\n",
        "    partial_dataset_bow = pd.concat([partial_dataset_bow, temp])\n",
        "\n",
        "# Reset count and drop unnamed column\n",
        "partial_dataset_bow = partial_dataset_bow.reset_index(drop=True)\n",
        "\n",
        "bow_tweets = partial_dataset_bow['Tweet'].to_numpy().tolist()\n",
        "bow_tweets_tok = [gensim.utils.simple_preprocess(array) for array in bow_tweets]\n",
        "d = gensim.corpora.Dictionary()\n",
        "bow_corpus = [d.doc2bow(sent, allow_update=True) for sent in bow_tweets_tok]\n",
        "bow_corpus"
      ],
      "metadata": {
        "id": "GIYY1vEaZqu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check corpus length\n",
        "max_cl = 0\n",
        "for item in bow_corpus:\n",
        "    for tup in item:\n",
        "        if tup[0] > max_cl:\n",
        "            max_cl = tup[0]\n",
        "\n",
        "max_cl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XJSb29DcDMp",
        "outputId": "fd69df42-dd99-4c43-9382-e72322adf0c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8785"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize\n",
        "vecs = []\n",
        "for item in bow_corpus:\n",
        "    vec = np.zeros((BOW_VEC_SIZE))\n",
        "    for tup in item:\n",
        "        if tup[0] < BOW_VEC_SIZE:\n",
        "            vec[tup[0]] = tup[1]\n",
        "    vecs.append(vec)\n",
        "vecs"
      ],
      "metadata": {
        "id": "wSrCiZFcgE35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill embeddings back in their correct place\n",
        "\n",
        "for i in range(len(vecs)):\n",
        "    partial_dataset_bow['Tweet'][i] = vecs[i]\n",
        "\n",
        "partial_dataset_bow"
      ],
      "metadata": {
        "id": "iDwH2vMpgptx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save bow embeddings\n",
        "partial_dataset_bow.to_pickle(f\"{input_dir}/bow_embeddings.pkl\")"
      ],
      "metadata": {
        "id": "VWsGkaRqg_HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load All Embeddings in One Place (for Clustering)"
      ],
      "metadata": {
        "id": "7qtQr9SsrkZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load W2V embeddings\n",
        "w2v_embeddings = pd.read_pickle(f\"{input_dir}/w2v_embeddings.pkl\")\n",
        "\n",
        "# Load BERT Embeddings\n",
        "BERT_embeddings = pd.read_pickle(f\"{input_dir}/BERT_Embeddings.pkl\")\n",
        "\n",
        "# Load bow embeddings\n",
        "bow_embeddings = pd.read_pickle(f\"{input_dir}/bow_embeddings.pkl\")"
      ],
      "metadata": {
        "id": "TCZNcrMTrj3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Means"
      ],
      "metadata": {
        "id": "Jk9nqjJXZc7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select embeddingg\n",
        "data_embeddings = w2v_embeddings\n",
        "#data_embeddings = BERT_embeddings\n",
        "#data_embeddings = bow_embeddings\n",
        "\n",
        "# Number of samples in each cluster\n",
        "# M = 8000 # w2v only\n",
        "\n",
        "M = 1000\n",
        "\n",
        "df = data_embeddings.groupby('Multi-Class Annotation').apply(lambda x:x.sample(M,random_state=0))\n",
        "# all classes ['ðŸ¤£', 'ðŸ˜‚', 'ðŸ˜…', 'ðŸ‘', 'â¤']\n",
        "Classes = ['ðŸ¤£', 'ðŸ˜‚', 'ðŸ˜…', 'ðŸ‘', 'â¤']\n",
        "subdf = df[df['Multi-Class Annotation'].apply(lambda x:x in Classes)]\n",
        "X = subdf['Tweet'].to_numpy()\n",
        "Y = subdf['Multi-Class Annotation'].to_numpy()\n",
        "X = np.vstack(X)\n",
        "\n",
        "from itertools import permutations\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def calc_acc(ypred): \n",
        "    label_perms = permutations(set(range(len(Classes))))\n",
        "    max_acc = 0\n",
        "    max_f1 = 0\n",
        "    for perm in list(label_perms):\n",
        "        yperm = np.repeat(perm, M)\n",
        "        assert(len(ypred) == len(yperm))\n",
        "        perm_acc = np.mean(ypred == yperm)\n",
        "        max_acc = max(max_acc, perm_acc)\n",
        "        perm_f1 = f1_score(yperm, ypred, average='weighted')\n",
        "        max_f1 = max(max_f1, perm_f1)\n",
        "    return [max_acc, max_f1] "
      ],
      "metadata": {
        "id": "yIrPOUncYaMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hierachical"
      ],
      "metadata": {
        "id": "ZYYwMv-7nnmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.cluster import AgglomerativeClustering\n",
        "# agglo = AgglomerativeClustering(n_clusters=len(Classes), linkage='average').fit(X)\n",
        "# print(calc_acc(agglo.labels_))\n",
        "\n",
        "# ward = AgglomerativeClustering(n_clusters=len(Classes), linkage='ward').fit(X)\n",
        "# print(calc_acc(ward.labels_))"
      ],
      "metadata": {
        "id": "XXDK2FpLnsrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spectral"
      ],
      "metadata": {
        "id": "c6_LaNhdnusI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.cluster import SpectralClustering\n",
        "# # spectral = SpectralClustering(n_clusters=len(Classes), affinity='nearest_neighbors', random_state=0).fit(X)\n",
        "# spectral = SpectralClustering(n_clusters=len(Classes), gamma=1e-4, random_state=0).fit(X)\n",
        "# print(calc_acc(spectral.labels_))"
      ],
      "metadata": {
        "id": "6tpl_R-gnuVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some 2D PCA"
      ],
      "metadata": {
        "id": "xEFlqpFwcfQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit(X).transform(X)\n",
        "\n",
        "plt.figure()\n",
        "colors = []\n",
        "for c in Classes:\n",
        "    if c == 'ðŸ˜‚':\n",
        "        colors.append(\"navy\")\n",
        "    elif c == 'ðŸ¤£':\n",
        "        colors.append(\"turquoise\")\n",
        "    elif c == 'ðŸ˜…':\n",
        "        colors.append(\"green\")\n",
        "    elif c == 'â¤' or c == 'ðŸ˜­':\n",
        "        colors.append(\"darkorange\")\n",
        "    elif c == 'ðŸ‘' or c == 'ðŸ˜”':\n",
        "        colors.append(\"red\")\n",
        "    else:\n",
        "        raise Exception(\"Unexpected class label\")\n",
        "# colors = [\"navy\", \"darkorange\", \"turquoise\", \"red\", \"green\"]\n",
        "\n",
        "for color, c in zip(colors, Classes):\n",
        "    plt.scatter(\n",
        "        X_pca[Y==c, 0], X_pca[Y==c, 1], c=color, label=c, alpha=0.8, s=2\n",
        "    )\n",
        "plt.legend(loc=\"best\", shadow=False, scatterpoints=1)\n",
        "plt.title(\"PCA\")\n"
      ],
      "metadata": {
        "id": "aZAO0Bntcmd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some 3D PCA"
      ],
      "metadata": {
        "id": "msepOvdsnRBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X) \n",
        "X_scaled = scaler.transform(X)\n",
        "\n",
        "pca = PCA(n_components=3)\n",
        "pca.fit(X_scaled) \n",
        "X_pca = pca.transform(X_scaled) \n",
        "\n",
        "Xax = X_pca[:,0]\n",
        "Yax = X_pca[:,1]\n",
        "Zax = X_pca[:,2]\n",
        "\n",
        "fig = plt.figure(figsize=(7,5))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "fig.patch.set_facecolor('white')\n",
        "\n",
        "# colors = [\"navy\", \"darkorange\", \"turquoise\", \"red\", \"green\"]\n",
        "for color, c in zip(colors, Classes):\n",
        "    ix = np.where(Y == c)\n",
        "    ax.scatter(Xax[ix], Yax[ix], Zax[ix], c=color, label=c, alpha=0.8, s=5)\n",
        "\n",
        "\n",
        "ax.set_xlabel(\"PC1\", fontsize=10)\n",
        "ax.set_ylabel(\"PC2\", fontsize=10)\n",
        "ax.set_zlabel(\"PC3\", fontsize=10)\n",
        "\n",
        "# ax.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3zHPl9HUnQlY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}